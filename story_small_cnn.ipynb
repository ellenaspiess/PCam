{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCam Small CNN: Step-by-step walkthrough\n",
    "- Goal: establish a baseline, test short hypotheses, then prepare hyperparameter tuning and a final run.\n",
    "- We start small (CPU, limited split), observe what works, and scale up gradually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Setup & Assumptions\n",
    "- CPU run for debugging; limited dataset for fast iteration.\n",
    "- Metric focus: val AUROC (primary), AUPRC (secondary). Loss used for stability checks.\n",
    "- Initial hypothesis: a small CNN will generalize with a moderate LR and light weight decay; batch size affects stability on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcc7254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "import optuna\n",
    "\n",
    "from src.datasets.dataloaders import get_pcam_dataloaders\n",
    "from src.models.small_cnn import SmallCNN\n",
    "from src.training.train_small_cnn import evaluate\n",
    "\n",
    "# Device setting: force CPU for debugging; switch to \"cuda\" for GPU later\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "DATA_ROOT = Path(\"data/raw\")\n",
    "CENTER_CROP = 64\n",
    "\n",
    "# Base settings for quick runs (adjusted further below)\n",
    "LIMIT_DEBUG = 512     # None for full split; keep small for CPU iterations\n",
    "NUM_WORKERS = 0       # CPU-friendly\n",
    "EPOCHS_DEBUG = 2\n",
    "BATCH_DEBUG = 32\n",
    "LR_DEBUG = 1e-3\n",
    "WD_DEBUG = 1e-4\n",
    "\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2209b2d",
   "metadata": {},
   "source": [
    "Why these defaults?\n",
    "- `LIMIT_DEBUG=512`: minimizes I/O and compute time, enough for coarse trends.\n",
    "- `LR_DEBUG=1e-3`: a robust starting point for Adam with small CNNs; often works without divergence.\n",
    "- `WD_DEBUG=1e-4`: light regularization to prevent overfitting on small splits.\n",
    "- `BATCH_DEBUG=32`: balance between stability and CPU speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb274042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loaders(batch_size: int, limit: int | None) -> Dict[str, torch.utils.data.DataLoader]:\n",
    "    return get_pcam_dataloaders(\n",
    "        data_root=DATA_ROOT,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        center_crop_size=CENTER_CROP,\n",
    "        limit_per_split=limit,\n",
    "    )\n",
    "\n",
    "\n",
    "def train_one_run(\n",
    "    batch_size: int,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    epochs: int,\n",
    "    limit: int | None,\n",
    "    dropout_p: float = 0.1,\n",
    "    patience: int = 2,\n",
    "    use_scheduler: bool = True,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    loaders = make_loaders(batch_size, limit)\n",
    "    model = SmallCNN(dropout_p=dropout_p).to(DEVICE)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = (\n",
    "        torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.5, patience=1, verbose=verbose\n",
    "        )\n",
    "        if use_scheduler\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_state = None\n",
    "    bad_epochs = 0\n",
    "    history = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, labels in loaders[\"train\"]:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.float().to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "        train_loss /= len(loaders[\"train\"].dataset)\n",
    "        val_loss, val_auroc, val_auprc = evaluate(model, loaders[\"val\"], criterion, DEVICE)\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        history.append(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_auroc\": val_auroc,\n",
    "                \"val_auprc\": val_auprc,\n",
    "                \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        improved = val_loss < best_val_loss\n",
    "        if improved:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "            bad_epochs = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | \"\n",
    "                f\"val_loss={val_loss:.4f} | AUROC={val_auroc:.3f} | AUPRC={val_auprc:.3f} | lr={optimizer.param_groups[0]['lr']:.2e}\"\n",
    "            )\n",
    "\n",
    "        if bad_epochs > patience:\n",
    "            if verbose:\n",
    "                print(f\"Early stopping at epoch {epoch} (no val improvement for {bad_epochs} epochs)\")\n",
    "            break\n",
    "\n",
    "    return history, best_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f753d771",
   "metadata": {},
   "source": [
    "## 2) Baseline on a small split\n",
    "Hypothesis: with moderate LR/WD we expect reasonable AUROC > 0.8 after a few epochs on 512 samples. If unstable, adjust LR or batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33902b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=0.6154 | val_loss=0.6913 | AUROC=0.756 | AUPRC=0.771\n",
      "Epoch 02 | train_loss=0.5331 | val_loss=0.5240 | AUROC=0.843 | AUPRC=0.813\n"
     ]
    }
   ],
   "source": [
    "DO_BASELINE = True\n",
    "if DO_BASELINE:\n",
    "    hist_baseline, _ = train_one_run(\n",
    "        batch_size=BATCH_DEBUG,\n",
    "        lr=LR_DEBUG,\n",
    "        weight_decay=WD_DEBUG,\n",
    "        epochs=EPOCHS_DEBUG,\n",
    "        limit=LIMIT_DEBUG,\n",
    "    )\n",
    "    hist_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b5574",
   "metadata": {},
   "source": [
    "Interpreting the baseline (manually after the run):\n",
    "- If AUROC quickly exceeds 0.8, the model learns stably → we can fine-tune.\n",
    "- If loss fluctuates or AUROC stays below 0.7 → reduce LR (e.g. 5e-4) or increase batch size (if using GPU); on CPU, smaller batches often give more stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36412a9a",
   "metadata": {},
   "source": [
    "## 3) Targeted tuning (Optuna, small search space)\n",
    "Idea: small search on CPU with pruning to probe LR / weight decay / batch size.\n",
    "- Search space: `lr` 1e-4..8e-4 (log), `weight_decay` 1e-5..1e-3 (log), `batch_size` {16, 32} for CPU.\n",
    "- Keep trials small (5–10) and use `LIMIT_DEBUG`.\n",
    "- Disabled by default; enable Optuna when you want to run the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e842becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_OPTUNA = False\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 8e-4, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32])\n",
    "\n",
    "    loaders = make_loaders(batch_size, LIMIT_DEBUG)\n",
    "    model = SmallCNN().to(DEVICE)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(1, EPOCHS_DEBUG + 1):\n",
    "        model.train()\n",
    "        for images, labels in loaders[\"train\"]:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.float().to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        _, val_auroc, _ = evaluate(model, loaders[\"val\"], criterion, DEVICE)\n",
    "        trial.report(val_auroc, step=epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return val_auroc\n",
    "\n",
    "if DO_OPTUNA:\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=\"small_cnn_story_cpu\",\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=1, n_warmup_steps=0),\n",
    "    )\n",
    "    study.optimize(objective, n_trials=5)\n",
    "    print(\"Best AUROC:\", study.best_value)\n",
    "    print(\"Best params:\", study.best_params)\n",
    "    trials_df = study.trials_dataframe()\n",
    "    trials_df[[\"number\", \"value\", \"state\", \"params_lr\", \"params_weight_decay\", \"params_batch_size\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6260e2",
   "metadata": {},
   "source": [
    "Interpreting the search results:\n",
    "- Choose the best parameters as a starting point for the larger run.\n",
    "- If the search shows little variation, widen the LR range or include `batch_size` 16/32/48 (if using GPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc700e7",
   "metadata": {},
   "source": [
    "**New anti-overfitting measures**:\n",
    "- Dropout 0.1 before the classifier in SmallCNN.\n",
    "- ReduceLROnPlateau on val loss (factor 0.5, patience=1) for LR adaptation.\n",
    "- Early stopping with patience=2 (stop if val does not improve).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e279d4",
   "metadata": {},
   "source": [
    "## 4) More complete run (more data, more epochs)\n",
    "Plan: test the best settings on more data and for more epochs.\n",
    "- On CPU: keep `LIMIT_FINAL` moderate (e.g. 2k–4k) and 6–10 epochs.\n",
    "- On GPU: `LIMIT_FINAL=None` (full split) and 10–15 epochs, optional LR scheduler (`ReduceLROnPlateau`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7d09a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example settings from a good trial\n",
    "LR_FINAL = 2.24e-4\n",
    "WD_FINAL = 4.28e-5\n",
    "BATCH_FINAL = 16\n",
    "EPOCHS_FINAL = 8\n",
    "LIMIT_FINAL = 2048   # None for full dataset; keep moderate on CPU\n",
    "\n",
    "DO_FINAL = False\n",
    "\n",
    "if DO_FINAL:\n",
    "    hist_final, _ = train_one_run(\n",
    "        batch_size=BATCH_FINAL,\n",
    "        lr=LR_FINAL,\n",
    "        weight_decay=WD_FINAL,\n",
    "        epochs=EPOCHS_FINAL,\n",
    "        limit=LIMIT_FINAL,\n",
    "    )\n",
    "    hist_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bfa564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=0.5387 | val_loss=0.4891 | AUROC=0.845 | AUPRC=0.814\n",
      "Epoch 02 | train_loss=0.5094 | val_loss=0.4794 | AUROC=0.860 | AUPRC=0.851\n",
      "Epoch 03 | train_loss=0.4949 | val_loss=0.4503 | AUROC=0.882 | AUPRC=0.877\n",
      "Epoch 04 | train_loss=0.4912 | val_loss=0.4538 | AUROC=0.875 | AUPRC=0.873\n",
      "Epoch 05 | train_loss=0.4863 | val_loss=0.4733 | AUROC=0.858 | AUPRC=0.874\n",
      "Epoch 06 | train_loss=0.4747 | val_loss=0.5090 | AUROC=0.870 | AUPRC=0.870\n",
      "Epoch 07 | train_loss=0.4689 | val_loss=0.4646 | AUROC=0.861 | AUPRC=0.861\n",
      "Epoch 08 | train_loss=0.4739 | val_loss=0.4250 | AUROC=0.893 | AUPRC=0.897\n",
      "Best epoch: 8 | train_loss=0.4739 | val_loss=0.4250 | AUROC=0.893 | AUPRC=0.897\n",
      "Saved model to experiments/runs/small_cnn_final.pt\n"
     ]
    }
   ],
   "source": [
    "# Final step: run final training, log results, save model\n",
    "DO_FINAL = True\n",
    "\n",
    "if DO_FINAL:\n",
    "    hist_final, best_state = train_one_run(\n",
    "        batch_size=BATCH_FINAL,\n",
    "        lr=LR_FINAL,\n",
    "        weight_decay=WD_FINAL,\n",
    "        epochs=EPOCHS_FINAL,\n",
    "        limit=LIMIT_FINAL,\n",
    "        verbose=True,\n",
    "    )\n",
    "    # Find best epoch (by AUROC)\n",
    "    best = max(hist_final, key=lambda x: x[\"val_auroc\"])\n",
    "    print(\n",
    "        f\"Best epoch: {best['epoch']} | \"\n",
    "        f\"train_loss={best['train_loss']:.4f} | \"\n",
    "        f\"val_loss={best['val_loss']:.4f} | \"\n",
    "        f\"AUROC={best['val_auroc']:.3f} | \"\n",
    "        f\"AUPRC={best['val_auprc']:.3f}\"\n",
    "    )\n",
    "\n",
    "    # Save model (best val-loss state)\n",
    "    if best_state is not None:\n",
    "        save_path = Path(\"experiments/runs/small_cnn_final.pt\")\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(best_state, save_path)\n",
    "        print(\"Saved model to\", save_path)\n",
    "    else:\n",
    "        print(\"Warn: best_state is None, nothing saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
