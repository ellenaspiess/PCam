{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCam Small CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0508c2c7",
   "metadata": {},
   "source": [
    "## 1) Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdcc7254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-25 01:13:42,670] A new study created in memory with name: no-name-2179e543-ec6b-4ea9-8c2f-b4b9ea508b50\n",
      "[I 2025-12-25 01:16:55,955] Trial 0 finished with value: 0.887096838709864 and parameters: {'lr': 0.0003843120994544614, 'batch_size': 64, 'weight_decay': 1.8765914810796102e-06}. Best is trial 0 with value: 0.887096838709864.\n",
      "[I 2025-12-25 01:20:05,877] Trial 1 finished with value: 0.8986096719819521 and parameters: {'lr': 0.0011692741755619618, 'batch_size': 128, 'weight_decay': 3.481434751802591e-05}. Best is trial 1 with value: 0.8986096719819521.\n",
      "[I 2025-12-25 01:23:17,125] Trial 2 finished with value: 0.8839527346234031 and parameters: {'lr': 0.00018885107643861452, 'batch_size': 128, 'weight_decay': 1.1462949543600632e-06}. Best is trial 1 with value: 0.8986096719819521.\n",
      "[I 2025-12-25 01:26:25,966] Trial 3 finished with value: 0.8802783790045154 and parameters: {'lr': 0.0014126816776050264, 'batch_size': 128, 'weight_decay': 6.23321038916348e-05}. Best is trial 1 with value: 0.8986096719819521.\n",
      "[I 2025-12-25 01:29:43,117] Trial 4 finished with value: 0.8897771964560978 and parameters: {'lr': 0.0003645597633495047, 'batch_size': 64, 'weight_decay': 6.065651167178675e-05}. Best is trial 1 with value: 0.8986096719819521.\n",
      "[I 2025-12-25 01:32:53,947] Trial 5 finished with value: 0.8850913929141255 and parameters: {'lr': 0.0003369052275136791, 'batch_size': 128, 'weight_decay': 0.0005432157433788979}. Best is trial 1 with value: 0.8986096719819521.\n",
      "[I 2025-12-25 01:36:02,724] Trial 6 finished with value: 0.8716333990205232 and parameters: {'lr': 0.00029466991283344935, 'batch_size': 128, 'weight_decay': 2.844286182286259e-06}. Best is trial 1 with value: 0.8986096719819521.\n",
      "[I 2025-12-25 01:39:19,119] Trial 7 finished with value: 0.8294486671066478 and parameters: {'lr': 0.002961677335058135, 'batch_size': 64, 'weight_decay': 0.0007687831536410104}. Best is trial 1 with value: 0.8986096719819521.\n",
      "[I 2025-12-25 01:42:29,464] Trial 8 finished with value: 0.8914174061963039 and parameters: {'lr': 0.0005012461470060974, 'batch_size': 64, 'weight_decay': 0.00010107136604657415}. Best is trial 1 with value: 0.8986096719819521.\n",
      "[I 2025-12-25 01:45:33,741] Trial 9 finished with value: 0.8798970829025696 and parameters: {'lr': 0.00020204500513054544, 'batch_size': 128, 'weight_decay': 6.924327487871024e-05}. Best is trial 1 with value: 0.8986096719819521.\n",
      "[I 2025-12-25 01:48:41,989] Trial 10 finished with value: 0.8874030345947699 and parameters: {'lr': 0.0011707708242471207, 'batch_size': 128, 'weight_decay': 7.741534809413715e-06}. Best is trial 1 with value: 0.8986096719819521.\n",
      "[I 2025-12-25 01:51:51,789] Trial 11 finished with value: 0.8870545935877754 and parameters: {'lr': 0.0007803853257204212, 'batch_size': 64, 'weight_decay': 1.7006401234633046e-05}. Best is trial 1 with value: 0.8986096719819521.\n",
      "[I 2025-12-25 01:55:06,215] Trial 12 finished with value: 0.8836682088011235 and parameters: {'lr': 0.00010309423219452053, 'batch_size': 64, 'weight_decay': 0.0001536310061789931}. Best is trial 1 with value: 0.8986096719819521.\n",
      "[I 2025-12-25 01:58:12,097] Trial 13 finished with value: 0.8765787983127271 and parameters: {'lr': 0.0024495895207304057, 'batch_size': 64, 'weight_decay': 1.813884434768058e-05}. Best is trial 1 with value: 0.8986096719819521.\n",
      "[I 2025-12-25 02:01:18,763] Trial 14 finished with value: 0.8864116367296301 and parameters: {'lr': 0.0007313302130331915, 'batch_size': 128, 'weight_decay': 0.0002127763503877036}. Best is trial 1 with value: 0.8986096719819521.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tuning fertig!\n",
      "Beste AUROC: 0.8986\n",
      "Beste Parameter: {'lr': 0.0011692741755619618, 'batch_size': 128, 'weight_decay': 3.481434751802591e-05}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from src.datasets.dataloaders import get_pcam_dataloaders\n",
    "from src.models.small_cnn_gpu import SmallCNN\n",
    "from src.training.utils_training import evaluate_binary_classifier\n",
    "\n",
    "# 1. Setup\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_ROOT = \"data/raw\"\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameter-Vorschl√§ge\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    \n",
    "    # Dataloader f√ºr Tuning (begrenztes Set f√ºr Speed)\n",
    "    # limit_per_split sorgt daf√ºr, dass die Trials schnell gehen\n",
    "    loaders = get_pcam_dataloaders(\n",
    "        data_root=DATA_ROOT,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0, # Im Notebook auf Windows sicherheitshalber 0\n",
    "        limit_per_split=20000 # ca. 8-10% des Datensatzes\n",
    "    )\n",
    "    \n",
    "    model = SmallCNN(dropout_p=0.2).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Kurzes Training f√ºr das Tuning (z.B. 5 Epochen)\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        for images, labels in loaders[\"train\"]:\n",
    "            images, labels = images.to(DEVICE), labels.float().to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    # Evaluation auf dem Validierungs-Set\n",
    "    _, val_auroc, _ = evaluate_binary_classifier(model, loaders[\"val\"], criterion, DEVICE)\n",
    "    return val_auroc\n",
    "\n",
    "# Studie starten\n",
    "study_cnn = optuna.create_study(direction=\"maximize\")\n",
    "study_cnn.optimize(objective, n_trials=15)\n",
    "\n",
    "print(\"‚úÖ Tuning fertig!\")\n",
    "print(f\"Beste AUROC: {study_cnn.best_value:.4f}\")\n",
    "print(f\"Beste Parameter: {study_cnn.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953f3276",
   "metadata": {},
   "source": [
    "## 2) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8d2ce75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starte finales Training: small_cnn_final_baseline\n",
      "[01/20] Train Loss: 0.4201 | Val AUROC: 0.8956 | Val AUPRC: 0.8912\n",
      "‚≠ê Modell gespeichert!\n",
      "[02/20] Train Loss: 0.3647 | Val AUROC: 0.8314 | Val AUPRC: 0.8673\n",
      "[03/20] Train Loss: 0.3295 | Val AUROC: 0.8592 | Val AUPRC: 0.8864\n",
      "[04/20] Train Loss: 0.3078 | Val AUROC: 0.9090 | Val AUPRC: 0.9191\n",
      "‚≠ê Modell gespeichert!\n",
      "[05/20] Train Loss: 0.2924 | Val AUROC: 0.9317 | Val AUPRC: 0.9324\n",
      "‚≠ê Modell gespeichert!\n",
      "[06/20] Train Loss: 0.2792 | Val AUROC: 0.9351 | Val AUPRC: 0.9385\n",
      "‚≠ê Modell gespeichert!\n",
      "[07/20] Train Loss: 0.2702 | Val AUROC: 0.9222 | Val AUPRC: 0.9308\n",
      "[08/20] Train Loss: 0.2633 | Val AUROC: 0.9124 | Val AUPRC: 0.9244\n",
      "[09/20] Train Loss: 0.2564 | Val AUROC: 0.9296 | Val AUPRC: 0.9334\n",
      "[10/20] Train Loss: 0.2511 | Val AUROC: 0.9215 | Val AUPRC: 0.9263\n",
      "[11/20] Train Loss: 0.2468 | Val AUROC: 0.9297 | Val AUPRC: 0.9316\n",
      "[12/20] Train Loss: 0.2432 | Val AUROC: 0.9389 | Val AUPRC: 0.9432\n",
      "‚≠ê Modell gespeichert!\n",
      "[13/20] Train Loss: 0.2398 | Val AUROC: 0.9409 | Val AUPRC: 0.9445\n",
      "‚≠ê Modell gespeichert!\n",
      "[14/20] Train Loss: 0.2359 | Val AUROC: 0.9320 | Val AUPRC: 0.9404\n",
      "[15/20] Train Loss: 0.2339 | Val AUROC: 0.9292 | Val AUPRC: 0.9373\n",
      "[16/20] Train Loss: 0.2313 | Val AUROC: 0.9081 | Val AUPRC: 0.9221\n",
      "[17/20] Train Loss: 0.2289 | Val AUROC: 0.9280 | Val AUPRC: 0.9349\n",
      "[18/20] Train Loss: 0.2261 | Val AUROC: 0.9204 | Val AUPRC: 0.9287\n",
      "[19/20] Train Loss: 0.2251 | Val AUROC: 0.9180 | Val AUPRC: 0.9310\n",
      "[20/20] Train Loss: 0.2231 | Val AUROC: 0.9226 | Val AUPRC: 0.9306\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def train_small_cnn_final(best_params, run_name, num_epochs=20):\n",
    "    print(f\"\\nüöÄ Starte finales Training: {run_name}\")\n",
    "    \n",
    "    # Full Dataloader (ohne Limit!)\n",
    "    loaders = get_pcam_dataloaders(\n",
    "        data_root=DATA_ROOT,\n",
    "        batch_size=best_params[\"batch_size\"],\n",
    "        num_workers=0, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    model = SmallCNN(dropout_p=0.2).to(DEVICE)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=best_params[\"lr\"], \n",
    "        weight_decay=best_params[\"weight_decay\"]\n",
    "    )\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    best_val_auprc = 0.0\n",
    "    save_path = Path(\"experiments/final_models\")\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, labels in loaders[\"train\"]:\n",
    "            images, labels = images.to(DEVICE), labels.float().to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Evaluation\n",
    "        avg_train_loss = train_loss / len(loaders[\"train\"].dataset)\n",
    "        val_loss, val_auroc, val_auprc = evaluate_binary_classifier(\n",
    "            model, loaders[\"val\"], criterion, DEVICE\n",
    "        )\n",
    "\n",
    "        print(f\"[{epoch:02d}/{num_epochs}] Train Loss: {avg_train_loss:.4f} | Val AUROC: {val_auroc:.4f} | Val AUPRC: {val_auprc:.4f}\")\n",
    "\n",
    "        # Speichern des besten Modells (nach AUPRC, wie im Feedback empfohlen)\n",
    "        if val_auprc > best_val_auprc:\n",
    "            best_val_auprc = val_auprc\n",
    "            torch.save(model.state_dict(), save_path / f\"{run_name}_best.pth\")\n",
    "            print(f\"‚≠ê Modell gespeichert!\")\n",
    "\n",
    "# Ausf√ºhren mit den Ergebnissen von oben\n",
    "train_small_cnn_final(study_cnn.best_params, \"small_cnn_final_baseline\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
