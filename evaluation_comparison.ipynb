{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d5edb47",
   "metadata": {},
   "source": [
    "## 1) Setup + Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be88da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, confusion_matrix\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Deine Module\n",
    "from src.datasets.dataloaders import get_pcam_dataloaders\n",
    "from src.models.small_cnn import SmallCNN\n",
    "from src.models.resnet_pcam import ResNetPCam, ResNetConfig\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Test-Daten laden (nur Test!)\n",
    "loaders = get_pcam_dataloaders(\n",
    "    data_root=\"data/raw\",\n",
    "    batch_size=64,\n",
    "    center_crop_size=64,\n",
    "    num_workers=0\n",
    ")\n",
    "test_loader = loaders[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b9822a",
   "metadata": {},
   "source": [
    "## 2) Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8f3525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_scores = [] # Wahrscheinlichkeiten (0.0 bis 1.0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            # Modell Output holen\n",
    "            logits = model(images)\n",
    "            probs = torch.sigmoid(logits) # Logits -> Wahrscheinlichkeit\n",
    "            \n",
    "            y_true.extend(labels.numpy())\n",
    "            y_scores.extend(probs.cpu().numpy())\n",
    "            \n",
    "    return np.array(y_true), np.array(y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9cbd20",
   "metadata": {},
   "source": [
    "## 3) Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78accc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A. Small CNN\n",
    "cnn_model = SmallCNN(dropout_p=0.5).to(device)\n",
    "cnn_model.load_state_dict(torch.load(\"experiments/models/small_cnn_best.pth\"))\n",
    "y_true, scores_cnn = get_predictions(cnn_model, test_loader)\n",
    "\n",
    "# B. ResNet Frozen\n",
    "cfg_frozen = ResNetConfig(tl_mode=\"frozen\", pretrained=True)\n",
    "resnet_frozen = ResNetPCam(cfg_frozen).to(device)\n",
    "resnet_frozen.load_state_dict(torch.load(\"experiments/models/resnet_frozen_best.pth\"))\n",
    "_, scores_frozen = get_predictions(resnet_frozen, test_loader) # y_true ist gleich\n",
    "\n",
    "# C. ResNet Partial\n",
    "cfg_partial = ResNetConfig(tl_mode=\"partial\", pretrained=True)\n",
    "resnet_partial = ResNetPCam(cfg_partial).to(device)\n",
    "resnet_partial.load_state_dict(torch.load(\"experiments/models/resnet_partial_best.pth\"))\n",
    "_, scores_partial = get_predictions(resnet_partial, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d26bd41",
   "metadata": {},
   "source": [
    "## 4) Plot 1: ROC Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Dictionary fÃ¼r einfache Schleife\n",
    "results = {\n",
    "    \"Small CNN\": scores_cnn,\n",
    "    \"ResNet (Frozen)\": scores_frozen,\n",
    "    \"ResNet (Partial)\": scores_partial\n",
    "}\n",
    "\n",
    "for name, scores in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_true, scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Zufall')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title('ROC Curves Comparison')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d023be",
   "metadata": {},
   "source": [
    "## 5) Plot 2: Precision-Recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf038d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, scores in results.items():\n",
    "    precision, recall, _ = precision_recall_curve(y_true, scores)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    plt.plot(recall, precision, label=f'{name} (AUPRC = {pr_auc:.3f})')\n",
    "\n",
    "plt.xlabel('Recall (Sensitivity)')\n",
    "plt.ylabel('Precision (Positive Predictive Value)')\n",
    "plt.title('Precision-Recall Comparison')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafe27e8",
   "metadata": {},
   "source": [
    "## 6) Table for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "data = []\n",
    "threshold = 0.5 # Standard-Grenzwert\n",
    "\n",
    "for name, scores in results.items():\n",
    "    preds = (scores >= threshold).astype(int)\n",
    "    \n",
    "    acc = accuracy_score(y_true, preds)\n",
    "    f1 = f1_score(y_true, preds)\n",
    "    roc_auc = roc_auc_score(y_true, scores)\n",
    "    pr_auc = average_precision_score(y_true, scores)\n",
    "    \n",
    "    data.append({\n",
    "        \"Model\": name,\n",
    "        \"AUROC\": roc_auc,\n",
    "        \"AUPRC\": pr_auc,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Accuracy\": acc\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "df.to_csv(\"experiments/results_simple_comparison.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
