{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCam Small CNN on Colab (GPU)\n",
    "End-to-end notebook for Colab with GPU: clone repo, install deps, download PCam, run training/Optuna.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before running:**\n",
    "- In Colab: `Runtime > Change runtime type > GPU`.\n",
    "- Set `REPO_URL` below (HTTPS clone)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, subprocess, sys\n",
    "\n",
    "# Set to your repo URL (HTTPS)\n",
    "REPO_URL = \"https://github.com/ellenaspiess/PCam.git\"\n",
    "REPO_DIR = pathlib.Path(\"/content/PCam\")\n",
    "\n",
    "if not REPO_DIR.exists():\n",
    "    assert REPO_URL and REPO_URL != \"<YOUR_REPO_URL>\", \"Please set REPO_URL\"\n",
    "    subprocess.run([\"git\", \"clone\", REPO_URL, str(REPO_DIR)], check=True)\n",
    "else:\n",
    "    # If rerun, pull latest changes\n",
    "    subprocess.run([\"git\", \"-C\", str(REPO_DIR), \"pull\"], check=True)\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "sys.path.insert(0, str(REPO_DIR))\n",
    "print(\"Working dir:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install minimal deps (Colab already has torch/torchvision, but we ensure versions)\n",
    "!pip install -q torch torchvision torchaudio scikit-learn optuna tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from src.datasets.dataloaders import get_pcam_dataloaders\n",
    "from src.training.train_small_cnn import train\n",
    "\n",
    "# Prefer GPU on Colab\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATA_ROOT = Path(\"data/raw\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# Quick download/verify (small subset) to warm cache\n",
    "loaders = get_pcam_dataloaders(\n",
    "    data_root=DATA_ROOT,\n",
    "    batch_size=16,\n",
    "    num_workers=2,\n",
    "    center_crop_size=64,\n",
    "    limit_per_split=128,\n",
    ")\n",
    "for k, v in loaders.items():\n",
    "    print(k, len(v.dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline / Final run\n",
    "Adjust `limit_per_split` to `None` for full dataset (longer on GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    data_root=DATA_ROOT,\n",
    "    num_epochs=8,\n",
    "    batch_size=32,\n",
    "    lr=2.24e-4,\n",
    "    device=DEVICE,\n",
    "    limit_per_split=None,  # set to None for full dataset; use small int for quick tests\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: small Optuna search (GPU)\n",
    "Keep trials modest to fit Colab session time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from src.models.small_cnn import SmallCNN\n",
    "from src.training.train_small_cnn import evaluate\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 8e-4, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 48])\n",
    "\n",
    "    loaders = get_pcam_dataloaders(\n",
    "        data_root=DATA_ROOT,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2,\n",
    "        center_crop_size=64,\n",
    "        limit_per_split=2048,  # reduce for speed\n",
    "    )\n",
    "\n",
    "    model = SmallCNN(dropout_p=0.1).to(DEVICE)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    EPOCHS = 3\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        for images, labels in loaders[\"train\"]:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.float().to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        _, val_auroc, _ = evaluate(model, loaders[\"val\"], criterion, DEVICE)\n",
    "        trial.report(val_auroc, step=epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    return val_auroc\n",
    "\n",
    "DO_OPTUNA = False  # set True to run\n",
    "if DO_OPTUNA:\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=1),\n",
    "    )\n",
    "    study.optimize(objective, n_trials=5)\n",
    "    print(\"Best AUROC:\", study.best_value)\n",
    "    print(\"Best params:\", study.best_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
