{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15cbb806",
   "metadata": {},
   "source": [
    "## 1) Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23984eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "# Eigene Module importieren\n",
    "from src.datasets.pcam_dataset import get_pcam_datasets\n",
    "from src.models.small_cnn_gpu import SmallCNN\n",
    "from src.models.resnet_pcam_gpu import ResNetPCam, ResNetConfig\n",
    "from src.training.utils_training import evaluate_binary_classifier\n",
    "\n",
    "# HIER IMPORTIEREN WIR DEINE DATEI\n",
    "from src.training.imbalance import make_imbalanced_subset\n",
    "\n",
    "# Device Setup\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸš€ Running on {DEVICE}\")\n",
    "\n",
    "DATA_ROOT = \"data/raw\"\n",
    "BATCH_SIZE = 128 \n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b67892",
   "metadata": {},
   "source": [
    "## 2) Data Preparation (10% Imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd60537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volle Datasets laden\n",
    "full_datasets = get_pcam_datasets(DATA_ROOT, center_crop_size=64)\n",
    "\n",
    "# Deine Funktion aus imbalance.py nutzen\n",
    "imbalanced_train_ds, SUGGESTED_WEIGHT = make_imbalanced_subset(\n",
    "    full_datasets[\"train\"], \n",
    "    target_ratio=0.1, \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Dataloader (Notebook-safe: num_workers=0 fÃ¼r Windows)\n",
    "train_loader = DataLoader(imbalanced_train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(full_datasets[\"val\"], batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdff0095",
   "metadata": {},
   "source": [
    "## 3) Training  & PR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c44690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_get_probs(model, criterion, optimizer, name):\n",
    "    print(f\"\\n--- Training: {name} ---\")\n",
    "\n",
    "    patience = 4  # Nach 4 Epochen ohne Verbesserung abbrechen\n",
    "    epochs_without_improvement = 0\n",
    "    best_val_auprc = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        # --- TRAINING PHASE ---\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(DEVICE), labels.float().to(DEVICE).view(-1, 1)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        # --- VALIDATION PHASE (fÃ¼r Early Stopping) ---\n",
    "        # Wir nutzen deine vorhandene evaluate_binary_classifier Funktion\n",
    "        val_loss, val_auroc, val_auprc = evaluate_binary_classifier(\n",
    "            model, val_loader, criterion, DEVICE\n",
    "        )\n",
    "        \n",
    "        print(f\"Epoch {epoch:02d}: Val-AUPRC = {val_auprc:.4f} (Best: {best_val_auprc:.4f})\")\n",
    "\n",
    "        # --- EARLY STOPPING LOGIK ---\n",
    "        if val_auprc > best_val_auprc:\n",
    "            best_val_auprc = val_auprc\n",
    "            epochs_without_improvement = 0\n",
    "            # Wir speichern den Zustand des besten Modells im RAM\n",
    "            import copy\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"â­ Verbesserung! Modell gemerkt.\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"âš ï¸ Keine Verbesserung ({epochs_without_improvement}/{patience})\")\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"ðŸ›‘ Early Stopping! Training abgebrochen in Epoche {epoch}\")\n",
    "            break\n",
    "\n",
    "    # --- FINISH ---\n",
    "    # Lade das beste Modell zurÃ¼ck, bevor wir die Vorhersagen generieren\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Vorhersagen fÃ¼r den PR-Plot generieren\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = torch.sigmoid(model(images.to(DEVICE)))\n",
    "            all_probs.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            \n",
    "    return np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "results_data = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b06187d",
   "metadata": {},
   "source": [
    "## 4) Run 4 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0481d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Small CNN Standard\n",
    "m1 = SmallCNN(dropout_p=0.2).to(DEVICE)\n",
    "y_true, p1 = train_and_get_probs(m1, nn.BCEWithLogitsLoss(), optim.Adam(m1.parameters(), lr=1e-3), \"CNN_Std\")\n",
    "results_data[\"CNN_Standard\"] = (y_true, p1)\n",
    "\n",
    "# 2. Small CNN Weighted\n",
    "m2 = SmallCNN(dropout_p=0.2).to(DEVICE)\n",
    "w = torch.tensor([SUGGESTED_WEIGHT]).to(DEVICE)\n",
    "y_true, p2 = train_and_get_probs(m2, nn.BCEWithLogitsLoss(pos_weight=w), optim.Adam(m2.parameters(), lr=1e-3), \"CNN_Weighted\")\n",
    "results_data[\"CNN_Weighted\"] = (y_true, p2)\n",
    "\n",
    "# 3. ResNet Standard\n",
    "m3 = ResNetPCam(ResNetConfig(tl_mode=\"partial\")).to(DEVICE)\n",
    "y_true, p3 = train_and_get_probs(m3, nn.BCEWithLogitsLoss(), optim.Adam(m3.parameters(), lr=1e-4), \"ResNet_Std\")\n",
    "results_data[\"ResNet_Standard\"] = (y_true, p3)\n",
    "\n",
    "# 4. ResNet Weighted\n",
    "m4 = ResNetPCam(ResNetConfig(tl_mode=\"partial\")).to(DEVICE)\n",
    "y_true, p4 = train_and_get_probs(m4, nn.BCEWithLogitsLoss(pos_weight=w), optim.Adam(m4.parameters(), lr=1e-4), \"ResNet_Weighted\")\n",
    "results_data[\"ResNet_Weighted\"] = (y_true, p4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2237786a",
   "metadata": {},
   "source": [
    "## 5) Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b61ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "for name, (y_t, y_p) in results_data.items():\n",
    "    precision, recall, _ = precision_recall_curve(y_t, y_p)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    plt.plot(recall, precision, label=f\"{name} (AUC={pr_auc:.3f})\")\n",
    "\n",
    "plt.xlabel(\"Recall (Anteil gefundener Metastasen)\")\n",
    "plt.ylabel(\"Precision (Genauigkeit der Alarme)\")\n",
    "plt.title(\"Woche 3 Vergleich: Impact von Loss-Weighting bei 10% Imbalance\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
