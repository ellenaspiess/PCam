{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfe46bbb",
   "metadata": {},
   "source": [
    "# ResNet18 Transfer Learning on PCam\n",
    "\n",
    "In this notebook I explore transfer learning with ResNet18 on the PatchCamelyon (PCam) dataset.\n",
    "\n",
    "Goals:\n",
    "- Use the same PCam dataloaders and preprocessing as for the SmallCNN.\n",
    "- Compare different transfer learning modes:\n",
    "  - **frozen**: only the final classification head is trainable\n",
    "  - **partial**: last ResNet block + head are trainable\n",
    "- Evaluate models using **AUROC** and **AUPRC** on the validation set.\n",
    "- Prepare results for later comparison with the SmallCNN notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4006c517",
   "metadata": {},
   "source": [
    "## 1) Setup: imports, project root, device, seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32d865c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h5py wurde erfolgreich manuell registriert!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# 1. h5py erzwingen (installieren, falls es doch fehlt)\n",
    "try:\n",
    "    import h5py\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"h5py\"])\n",
    "    import h5py\n",
    "\n",
    "# 2. Den Patch anwenden, damit das Dataset-Objekt h5py findet\n",
    "from torchvision.datasets import PCAM\n",
    "PCAM.h5py = h5py\n",
    "\n",
    "print(\"h5py wurde erfolgreich manuell registriert!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04709753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\felix\\Documents\\Ellena\\PCam\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Make sure we can import from the project src/ folder\n",
    "# -------------------------------------------------------------------\n",
    "ROOT = Path().resolve()\n",
    "if not (ROOT / \"src\").exists():\n",
    "    # If the notebook is inside notebooks/, go one level up\n",
    "    ROOT = ROOT.parent\n",
    "    os.chdir(ROOT)\n",
    "\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "\n",
    "print(\"Project root:\", ROOT)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Reproducibility helpers\n",
    "# -------------------------------------------------------------------\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"Select best available device: CUDA, MPS (Apple), or CPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "DEVICE = get_device()\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b6fc53",
   "metadata": {},
   "source": [
    "## 2) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a0a294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches:\n",
      "  train: 32768\n",
      "  val: 4096\n",
      "  test: 4096\n"
     ]
    }
   ],
   "source": [
    "from src.datasets.dataloaders import get_pcam_dataloaders\n",
    "\n",
    "# You can adjust batch_size and crop size if needed\n",
    "BATCH_SIZE = 64\n",
    "CENTER_CROP = 64\n",
    "\n",
    "loaders = get_pcam_dataloaders(data_root=\"data/raw\", batch_size=8, center_crop_size=64, num_workers=0, pin_memory=False)\n",
    "\n",
    "print(\"Number of batches:\")\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    print(f\"  {split}: {len(loaders[split])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eec706",
   "metadata": {},
   "source": [
    "## 3) Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a8e6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([8, 3, 64, 64])\n",
      "Labels shape: torch.Size([8])\n",
      "First 10 labels: tensor([0, 1, 1, 0, 0, 1, 1, 0])\n",
      "Label distribution in this batch: {0: 4, 1: 4}\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(loaders[\"train\"]))\n",
    "print(\"Images shape:\", images.shape)   # [B, 3, 64, 64]\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "print(\"First 10 labels:\", labels[:10])\n",
    "\n",
    "unique, counts = torch.unique(labels, return_counts=True)\n",
    "print(\"Label distribution in this batch:\", dict(zip(unique.tolist(), counts.tolist())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda40284",
   "metadata": {},
   "source": [
    "## 4) Import ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1a2e75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode='frozen': trainable params = 513\n",
      "Mode='partial': trainable params = 8,394,241\n",
      "Mode='full': trainable params = 11,177,025\n"
     ]
    }
   ],
   "source": [
    "from src.models.resnet_pcam_gpu import ResNetPCam, ResNetConfig\n",
    "\n",
    "def count_trainable_params(model: torch.nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "for mode in [\"frozen\", \"partial\", \"full\"]:\n",
    "    cfg = ResNetConfig(tl_mode=mode, pretrained=True)\n",
    "    m = ResNetPCam(cfg)\n",
    "    print(\n",
    "        f\"Mode='{mode}': trainable params = {count_trainable_params(m):,}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb15353",
   "metadata": {},
   "source": [
    "## 5) Tuning Hyperparameters with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae21902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "\n",
    "def get_tuning_dataloaders(\n",
    "    full_train_ds,      # <--- Neu: Dataset wird Ã¼bergeben\n",
    "    full_val_ds,        # <--- Neu: Dataset wird Ã¼bergeben\n",
    "    batch_size: int,\n",
    "    max_train_samples: int = 25000, # <--- Empfehlung: ErhÃ¶ht auf ca 10%\n",
    "    max_val_samples: int = 5000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build smaller train/val dataloaders for hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generator fÃ¼r Reproduzierbarkeit (WICHTIG!)\n",
    "    g = torch.Generator().manual_seed(42)\n",
    "\n",
    "    # Indizes einmalig wÃ¼rfeln\n",
    "    # Wir nehmen min(), falls das Dataset kleiner ist als unser Limit\n",
    "    train_size = min(len(full_train_ds), max_train_samples)\n",
    "    val_size = min(len(full_val_ds), max_val_samples)\n",
    "\n",
    "    # ZufÃ¤llige Indizes auswÃ¤hlen\n",
    "    train_indices = torch.randperm(len(full_train_ds), generator=g)[:train_size]\n",
    "    val_indices = torch.randperm(len(full_val_ds), generator=g)[:val_size]\n",
    "\n",
    "    # Subsets erstellen\n",
    "    train_subset = Subset(full_train_ds, train_indices.tolist())\n",
    "    val_subset = Subset(full_val_ds, val_indices.tolist())\n",
    "\n",
    "    # Loaders bauen\n",
    "    # Tipp: Wenn du auf GPU bist, setz num_workers=4 (auÃŸer auf Windows mit Problemen)\n",
    "    train_loader = DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0, # Ggf. auf 4 erhÃ¶hen fÃ¼r Speed\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0, # Ggf. auf 4 erhÃ¶hen\n",
    "        pin_memory=True,\n",
    "\n",
    "\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f5c548e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Datasets fÃ¼r Tuning (einmalig)...\n",
      "âœ… Datasets bereit.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from src.datasets.dataloaders import get_pcam_dataloaders\n",
    "from src.models.resnet_pcam_gpu import ResNetConfig, ResNetPCam\n",
    "from src.training.utils_training import evaluate_binary_classifier\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP: Daten EINMALIG laden (WICHTIG!)\n",
    "# ==========================================\n",
    "# Wir erstellen hier die \"Eltern-Datasets\".\n",
    "# Optuna erstellt spÃ¤ter nur noch leichte Subsets davon.\n",
    "# center_crop_size wird hier festgelegt!\n",
    "print(\"Lade Datasets fÃ¼r Tuning (einmalig)...\")\n",
    "_temp_loaders = get_pcam_dataloaders(\n",
    "    data_root=\"data/raw\", \n",
    "    batch_size=1,            # Hier egal, wir brauchen nur das Dataset\n",
    "    center_crop_size=64,     # Hier wird der Crop fixiert!\n",
    "    num_workers=0\n",
    ")\n",
    "FULL_TRAIN_DS = _temp_loaders[\"train\"].dataset\n",
    "FULL_VAL_DS = _temp_loaders[\"val\"].dataset\n",
    "print(\"âœ… Datasets bereit.\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Helper Funktionen\n",
    "# ==========================================\n",
    "def build_resnet_and_optimizer(tl_mode: str, lr: float, weight_decay: float):\n",
    "    \"\"\"Baut Modell und Optimizer fÃ¼r einen Trial.\"\"\"\n",
    "    cfg = ResNetConfig(tl_mode=tl_mode, pretrained=True)\n",
    "    model = ResNetPCam(cfg).to(DEVICE) # DEVICE muss global definiert sein (z.B. \"cuda\")\n",
    "\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    \n",
    "    optimizer = optim.Adam(\n",
    "        trainable_params,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    return model, optimizer, criterion\n",
    "\n",
    "# Globaler Modus fÃ¼r die Studie (wird vor study.optimize gesetzt)\n",
    "FIXED_TL_MODE = \"frozen\" \n",
    "\n",
    "# ==========================================\n",
    "# 3. Die Optuna Objective Funktion\n",
    "# ==========================================\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Optimiert Hyperparameter auf einem Subset der Daten.\n",
    "    \"\"\"\n",
    "    tl_mode = FIXED_TL_MODE \n",
    "\n",
    "    # --- A. Hyperparameter vorschlagen ---\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64])\n",
    "\n",
    "    # --- B. Schnelle Dataloader holen ---\n",
    "    # Hier nutzen wir die neue Funktion und Ã¼bergeben die globalen Datasets\n",
    "    train_loader, val_loader = get_tuning_dataloaders(\n",
    "        FULL_TRAIN_DS,       # <--- Das globale Dataset nutzen\n",
    "        FULL_VAL_DS,         # <--- Das globale Dataset nutzen\n",
    "        batch_size=batch_size,\n",
    "        max_train_samples=20000, # <--- ErhÃ¶ht fÃ¼r StabilitÃ¤t (ca. 8-10%)\n",
    "        max_val_samples=5000,\n",
    "    )\n",
    "\n",
    "    # --- C. Modell Setup ---\n",
    "    model, optimizer, criterion = build_resnet_and_optimizer(\n",
    "        tl_mode=tl_mode,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "\n",
    "    # --- D. Kurzer Training Loop ---\n",
    "    num_epochs = 3  # 3 Epochen reichen oft fÃ¼r eine Tendenz\n",
    "    best_val_auroc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.float().to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Evaluation\n",
    "        val_loss, val_auroc, val_auprc = evaluate_binary_classifier(\n",
    "            model, val_loader, criterion, DEVICE\n",
    "        )\n",
    "\n",
    "        # Optuna Pruning (bricht aussichtslose Trials frÃ¼h ab)\n",
    "        trial.report(val_auroc, step=epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "        best_val_auroc = max(best_val_auroc, val_auroc)\n",
    "\n",
    "    return best_val_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af9096ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 10:19:05,508] A new study created in memory with name: resnet_pcam_frozen\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starte Optuna Study fÃ¼r Modus: 'frozen'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 10:21:57,124] Trial 0 finished with value: 0.8283977317439518 and parameters: {'lr': 5.6115164153345e-05, 'weight_decay': 0.0007969454818643932, 'batch_size': 32}. Best is trial 0 with value: 0.8283977317439518.\n",
      "[I 2025-12-24 10:24:47,323] Trial 1 finished with value: 0.7004907066242594 and parameters: {'lr': 2.0513382630874486e-05, 'weight_decay': 2.0511104188433963e-05, 'batch_size': 64}. Best is trial 0 with value: 0.8283977317439518.\n",
      "[I 2025-12-24 10:27:45,166] Trial 2 finished with value: 0.8514119156247665 and parameters: {'lr': 0.00015930522616241006, 'weight_decay': 0.0002607024758370766, 'batch_size': 64}. Best is trial 2 with value: 0.8514119156247665.\n",
      "[I 2025-12-24 10:30:41,063] Trial 3 finished with value: 0.8609873112646829 and parameters: {'lr': 0.000462258900102083, 'weight_decay': 2.6587543983272695e-05, 'batch_size': 64}. Best is trial 3 with value: 0.8609873112646829.\n",
      "[I 2025-12-24 10:33:48,055] Trial 4 finished with value: 0.8150335087643785 and parameters: {'lr': 4.059611610484306e-05, 'weight_decay': 0.00011207606211860574, 'batch_size': 32}. Best is trial 3 with value: 0.8609873112646829.\n",
      "[I 2025-12-24 10:36:55,029] Trial 5 finished with value: 0.8551923423716778 and parameters: {'lr': 0.00016738085788752134, 'weight_decay': 1.9010245319870364e-05, 'batch_size': 64}. Best is trial 3 with value: 0.8609873112646829.\n",
      "[I 2025-12-24 10:38:36,373] Trial 6 pruned. \n",
      "[I 2025-12-24 10:41:39,937] Trial 7 finished with value: 0.8572854500613564 and parameters: {'lr': 0.00015304852121831474, 'weight_decay': 1.2385137298860926e-05, 'batch_size': 32}. Best is trial 3 with value: 0.8609873112646829.\n",
      "[I 2025-12-24 10:43:38,749] Trial 8 pruned. \n",
      "[I 2025-12-24 10:45:39,766] Trial 9 pruned. \n",
      "[I 2025-12-24 10:48:34,360] Trial 10 finished with value: 0.86194225864577 and parameters: {'lr': 0.0007767353816237047, 'weight_decay': 5.3389437745561126e-05, 'batch_size': 64}. Best is trial 10 with value: 0.86194225864577.\n",
      "[I 2025-12-24 10:51:25,273] Trial 11 finished with value: 0.8594932058406042 and parameters: {'lr': 0.0008040489951543936, 'weight_decay': 4.9211947176147695e-05, 'batch_size': 64}. Best is trial 10 with value: 0.86194225864577.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "âœ… Fertig (Frozen)!\n",
      "Anzahl Trials: 12\n",
      "Beste AUROC: 0.8619\n",
      "Beste Parameter:\n",
      "  lr: 0.0007767353816237047\n",
      "  weight_decay: 5.3389437745561126e-05\n",
      "  batch_size: 64\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "# 1. Globale Variable setzen (WICHTIG!)\n",
    "# Die objective-Funktion greift auf diesen Wert zu.\n",
    "FIXED_TL_MODE = \"frozen\"\n",
    "\n",
    "print(f\"ðŸš€ Starte Optuna Study fÃ¼r Modus: '{FIXED_TL_MODE}'\")\n",
    "\n",
    "# 2. Sampler & Pruner definieren\n",
    "# TPESampler: Lernt aus vorherigen Versuchen (besser als Random Search)\n",
    "# MedianPruner: Bricht Versuche ab, die schlechter laufen als der Durchschnitt (spart Zeit!)\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1, interval_steps=1)\n",
    "\n",
    "# 3. Studie erstellen\n",
    "study_frozen = optuna.create_study(\n",
    "    study_name=\"resnet_pcam_frozen\",\n",
    "    direction=\"maximize\", # Wir wollen AUROC maximieren\n",
    "    sampler=sampler,\n",
    "    pruner=pruner,\n",
    ")\n",
    "\n",
    "# 4. Optimierung starten\n",
    "# n_trials=12 ist gut fÃ¼r den Start (dauert ca. 10-15 Min auf GPU)\n",
    "study_frozen.optimize(objective, n_trials=12)\n",
    "\n",
    "# 5. Ergebnisse ausgeben\n",
    "print(\"-\" * 50)\n",
    "print(\"âœ… Fertig (Frozen)!\")\n",
    "print(f\"Anzahl Trials: {len(study_frozen.trials)}\")\n",
    "print(f\"Beste AUROC: {study_frozen.best_trial.value:.4f}\")\n",
    "print(\"Beste Parameter:\")\n",
    "for k, v in study_frozen.best_trial.params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# 6. Beste Parameter speichern fÃ¼r spÃ¤teres Training\n",
    "best_params_frozen = study_frozen.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9fa5f1",
   "metadata": {},
   "source": [
    "âœ… Fertig (Frozen)!\n",
    "Anzahl Trials: 12\n",
    "Beste AUROC: 0.8619\n",
    "Beste Parameter:\n",
    "  lr: 0.0007767353816237047\n",
    "  weight_decay: 5.3389437745561126e-05\n",
    "  batch_size: 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6cb1374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 10:51:25,301] A new study created in memory with name: resnet_pcam_partial\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starte Optuna Study fÃ¼r Modus: 'partial'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 10:54:18,422] Trial 0 finished with value: 0.9078107751282931 and parameters: {'lr': 5.6115164153345e-05, 'weight_decay': 0.0007969454818643932, 'batch_size': 32}. Best is trial 0 with value: 0.9078107751282931.\n",
      "[I 2025-12-24 10:57:07,686] Trial 1 finished with value: 0.8993869807453615 and parameters: {'lr': 2.0513382630874486e-05, 'weight_decay': 2.0511104188433963e-05, 'batch_size': 64}. Best is trial 0 with value: 0.9078107751282931.\n",
      "[I 2025-12-24 11:00:11,132] Trial 2 finished with value: 0.9090135799982046 and parameters: {'lr': 0.00015930522616241006, 'weight_decay': 0.0002607024758370766, 'batch_size': 64}. Best is trial 2 with value: 0.9090135799982046.\n",
      "[I 2025-12-24 11:02:58,937] Trial 3 finished with value: 0.9199825139661855 and parameters: {'lr': 0.000462258900102083, 'weight_decay': 2.6587543983272695e-05, 'batch_size': 64}. Best is trial 3 with value: 0.9199825139661855.\n",
      "[I 2025-12-24 11:06:58,274] Trial 4 finished with value: 0.9117384922680144 and parameters: {'lr': 4.059611610484306e-05, 'weight_decay': 0.00011207606211860574, 'batch_size': 32}. Best is trial 3 with value: 0.9199825139661855.\n",
      "[I 2025-12-24 11:09:41,444] Trial 5 finished with value: 0.9148061487218537 and parameters: {'lr': 0.00016738085788752134, 'weight_decay': 1.9010245319870364e-05, 'batch_size': 64}. Best is trial 3 with value: 0.9199825139661855.\n",
      "[I 2025-12-24 11:12:32,491] Trial 6 pruned. \n",
      "[I 2025-12-24 11:16:33,662] Trial 7 finished with value: 0.9126728781982856 and parameters: {'lr': 0.00015304852121831474, 'weight_decay': 1.2385137298860926e-05, 'batch_size': 32}. Best is trial 3 with value: 0.9199825139661855.\n",
      "[I 2025-12-24 11:19:05,045] Trial 8 pruned. \n",
      "[I 2025-12-24 11:21:20,962] Trial 9 pruned. \n",
      "[I 2025-12-24 11:24:08,142] Trial 10 finished with value: 0.9098648400631149 and parameters: {'lr': 0.0007767353816237047, 'weight_decay': 5.3389437745561126e-05, 'batch_size': 64}. Best is trial 3 with value: 0.9199825139661855.\n",
      "[I 2025-12-24 11:26:58,485] Trial 11 finished with value: 0.9176217073876732 and parameters: {'lr': 0.0004860723886255582, 'weight_decay': 3.8204563909643186e-05, 'batch_size': 64}. Best is trial 3 with value: 0.9199825139661855.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "âœ… Fertig (partial)!\n",
      "Anzahl Trials: 12\n",
      "Beste AUROC: 0.9200\n",
      "Beste Parameter:\n",
      "  lr: 0.000462258900102083\n",
      "  weight_decay: 2.6587543983272695e-05\n",
      "  batch_size: 64\n"
     ]
    }
   ],
   "source": [
    "# 1. Modus auf PARTIAL Ã¤ndern\n",
    "FIXED_TL_MODE = \"partial\"\n",
    "\n",
    "print(f\"\\nðŸš€ Starte Optuna Study fÃ¼r Modus: '{FIXED_TL_MODE}'\")\n",
    "\n",
    "# 2. Sampler & Pruner (Konsistent halten: n_startup_trials=5)\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1, interval_steps=1)\n",
    "\n",
    "# 3. Studie erstellen\n",
    "study_partial = optuna.create_study(\n",
    "    study_name=\"resnet_pcam_partial\",\n",
    "    direction=\"maximize\",\n",
    "    sampler=sampler,\n",
    "    pruner=pruner,\n",
    ")\n",
    "\n",
    "# 4. Optimieren (Gleiches Budget wie Frozen)\n",
    "study_partial.optimize(objective, n_trials=12)\n",
    "\n",
    "# 5. Ergebnisse ausgeben\n",
    "print(\"-\" * 50)\n",
    "print(f\"âœ… Fertig ({FIXED_TL_MODE})!\")\n",
    "print(f\"Anzahl Trials: {len(study_partial.trials)}\")\n",
    "print(f\"Beste AUROC: {study_partial.best_trial.value:.4f}\")\n",
    "print(\"Beste Parameter:\")\n",
    "for k, v in study_partial.best_trial.params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Speichern fÃ¼r das finale Training\n",
    "best_params_partial = study_partial.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d4b5d5",
   "metadata": {},
   "source": [
    "âœ… Fertig (partial)!\n",
    "Anzahl Trials: 12\n",
    "Beste AUROC: 0.9200\n",
    "Beste Parameter:\n",
    "  lr: 0.000462258900102083\n",
    "  weight_decay: 2.6587543983272695e-05\n",
    "  batch_size: 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c4d837",
   "metadata": {},
   "source": [
    "## 5) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84c895d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training auf: cuda | Workers: 0\n",
      ">>> Starte Training A: FROZEN Baseline\n",
      "\n",
      "=== Starte Finales Training: resnet_frozen_final (frozen) ===\n",
      "ðŸ“Š DATEN-CHECK: Training auf 262144 Bildern | Validierung auf 32768 Bildern\n",
      "Trainable Params: 513\n",
      "âš¡ Mixed Precision (AMP) aktiviert.\n",
      "[01/10] Loss: 0.4875 | Val Loss: 0.4805 | AUROC: 0.8540 | AUPRC: 0.8463\n",
      "[02/10] Loss: 0.4778 | Val Loss: 0.4718 | AUROC: 0.8595 | AUPRC: 0.8524\n",
      "[03/10] Loss: 0.4778 | Val Loss: 0.4792 | AUROC: 0.8561 | AUPRC: 0.8489\n",
      "[04/10] Loss: 0.4772 | Val Loss: 0.4751 | AUROC: 0.8571 | AUPRC: 0.8492\n",
      "[05/10] Loss: 0.4773 | Val Loss: 0.4813 | AUROC: 0.8538 | AUPRC: 0.8448\n",
      "[06/10] Loss: 0.4722 | Val Loss: 0.4723 | AUROC: 0.8598 | AUPRC: 0.8517\n",
      "[07/10] Loss: 0.4705 | Val Loss: 0.4661 | AUROC: 0.8631 | AUPRC: 0.8572\n",
      "[08/10] Loss: 0.4700 | Val Loss: 0.4673 | AUROC: 0.8627 | AUPRC: 0.8554\n",
      "[09/10] Loss: 0.4689 | Val Loss: 0.4693 | AUROC: 0.8611 | AUPRC: 0.8558\n",
      "[10/10] Loss: 0.4695 | Val Loss: 0.4673 | AUROC: 0.8624 | AUPRC: 0.8562\n",
      "âœ… Training fertig! Bestes AUPRC: 0.8572\n",
      "--------------------------------------------------\n",
      ">>> Starte Training B: PARTIAL Fine-Tuning\n",
      "\n",
      "=== Starte Finales Training: resnet_partial_final (partial) ===\n",
      "ðŸ“Š DATEN-CHECK: Training auf 262144 Bildern | Validierung auf 32768 Bildern\n",
      "Trainable Params: 8,394,241\n",
      "âš¡ Mixed Precision (AMP) aktiviert.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\felix\\Documents\\Ellena\\PCam\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 151\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;66;03m# B) Training PARTIAL\u001b[39;00m\n\u001b[32m    150\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m>>> Starte Training B: PARTIAL Fine-Tuning\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m final_model_partial = \u001b[43mtrain_resnet_final\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtl_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpartial\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbest_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbest_params_partial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresnet_partial_final\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\n\u001b[32m    156\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mtrain_resnet_final\u001b[39m\u001b[34m(tl_mode, best_params, run_name, num_epochs, save_dir)\u001b[39m\n\u001b[32m    106\u001b[39m         loss.backward()\n\u001b[32m    107\u001b[39m         optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     train_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * images.size(\u001b[32m0\u001b[39m)\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[32m    112\u001b[39m train_loss /= \u001b[38;5;28mlen\u001b[39m(loaders[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m].dataset)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import importlib\n",
    "\n",
    "# Eigene Module (Reload stellt sicher, dass Ã„nderungen Ã¼bernommen werden)\n",
    "import src.models.resnet_pcam_gpu as resnet_module\n",
    "importlib.reload(resnet_module)\n",
    "from src.models.resnet_pcam_gpu import ResNetConfig, ResNetPCam\n",
    "from src.datasets.dataloaders import get_pcam_dataloaders\n",
    "from src.training.utils_training import evaluate_binary_classifier\n",
    "\n",
    "# ==========================================\n",
    "# 1. KONFIGURATION (Windows-Safe!)\n",
    "# ==========================================\n",
    "# WICHTIG: Im Notebook auf Windows MUSS das 0 sein.\n",
    "# Wenn du Speed willst, mÃ¼sstest du das Skript als .py Datei im Terminal starten.\n",
    "NUM_WORKERS = 0 \n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸš€ Training auf: {DEVICE} | Workers: {NUM_WORKERS}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Die Trainings-Funktion (ROBUST FIX)\n",
    "# ==========================================\n",
    "def train_resnet_final(\n",
    "    tl_mode: str,\n",
    "    best_params: dict,\n",
    "    run_name: str,\n",
    "    num_epochs: int = 15,\n",
    "    save_dir: str = \"experiments/final_models\"\n",
    "):\n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n=== Starte Finales Training: {run_name} ({tl_mode}) ===\")\n",
    "    \n",
    "    # Params aus Optuna holen\n",
    "    batch_size = best_params[\"batch_size\"]\n",
    "    lr = best_params[\"lr\"]\n",
    "    weight_decay = best_params[\"weight_decay\"]\n",
    "    \n",
    "    # Loader neu erstellen (Full Dataset)\n",
    "    # Wichtig: pin_memory entfernt & num_workers=0 fÃ¼r Windows\n",
    "    loaders = get_pcam_dataloaders(\n",
    "        data_root=\"data/raw\",\n",
    "        batch_size=batch_size,\n",
    "        center_crop_size=64, \n",
    "        num_workers=NUM_WORKERS \n",
    "    )\n",
    "    \n",
    "    # Sicherheits-Check\n",
    "    n_train = len(loaders[\"train\"].dataset)\n",
    "    n_val = len(loaders[\"val\"].dataset)\n",
    "    print(f\"ðŸ“Š DATEN-CHECK: Training auf {n_train} Bildern | Validierung auf {n_val} Bildern\")\n",
    "\n",
    "    # Modell Setup\n",
    "    cfg = ResNetConfig(tl_mode=tl_mode, pretrained=True)\n",
    "    model = ResNetPCam(cfg).to(DEVICE)\n",
    "    \n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    print(f\"Trainable Params: {sum(p.numel() for p in trainable_params):,}\")\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(trainable_params, lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=2, verbose=True)\n",
    "    \n",
    "    # FIX: Klassischer Weg fÃ¼r Mixed Precision (funktioniert immer)\n",
    "    scaler = None\n",
    "    if DEVICE.type == 'cuda':\n",
    "        try:\n",
    "            scaler = torch.cuda.amp.GradScaler()\n",
    "            print(\"âš¡ Mixed Precision (AMP) aktiviert.\")\n",
    "        except Exception:\n",
    "            print(\"âš ï¸ Konnte AMP nicht aktivieren, laufe in Standard-PrÃ¤zision.\")\n",
    "    \n",
    "    best_val_auprc = 0.0 \n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for images, labels in loaders[\"train\"]:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.float().to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Training Step\n",
    "            if scaler:\n",
    "                # Klassischer Autocast Context\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    logits = model(images)\n",
    "                    loss = criterion(logits, labels)\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Standard ohne AMP\n",
    "                logits = model(images)\n",
    "                loss = criterion(logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Evaluation\n",
    "        train_loss /= len(loaders[\"train\"].dataset)\n",
    "        val_loss, val_auroc, val_auprc = evaluate_binary_classifier(\n",
    "            model, loaders[\"val\"], criterion, DEVICE\n",
    "        )\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(\n",
    "            f\"[{epoch:02d}/{num_epochs}] \"\n",
    "            f\"Loss: {train_loss:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f} | \"\n",
    "            f\"AUROC: {val_auroc:.4f} | \"\n",
    "            f\"AUPRC: {val_auprc:.4f}\"\n",
    "        )\n",
    "\n",
    "        if val_auprc > best_val_auprc:\n",
    "            best_val_auprc = val_auprc\n",
    "            torch.save(model.state_dict(), save_path / f\"{run_name}_best.pth\")\n",
    "            \n",
    "    print(f\"âœ… Training fertig! Bestes AUPRC: {best_val_auprc:.4f}\")\n",
    "    return model\n",
    "\n",
    "# ==========================================\n",
    "# 3. AUSFÃœHRUNG STARTEN\n",
    "# ==========================================\n",
    "\n",
    "# A) Training FROZEN\n",
    "print(\">>> Starte Training A: FROZEN Baseline\")\n",
    "final_model_frozen = train_resnet_final(\n",
    "    tl_mode=\"frozen\",\n",
    "    best_params=best_params_frozen,\n",
    "    run_name=\"resnet_frozen_final\",\n",
    "    num_epochs=10\n",
    ")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# B) Training PARTIAL\n",
    "print(\">>> Starte Training B: PARTIAL Fine-Tuning\")\n",
    "final_model_partial = train_resnet_final(\n",
    "    tl_mode=\"partial\",\n",
    "    best_params=best_params_partial,\n",
    "    run_name=\"resnet_partial_final\",\n",
    "    num_epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4164b07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training auf: cuda | Workers: 0\n",
      ">>> Starte Training B: PARTIAL Fine-Tuning\n",
      "\n",
      "=== Starte Finales Training: resnet_partial_final (partial) ===\n",
      "ðŸ“Š DATEN-CHECK: Training auf 262144 Bildern | Validierung auf 32768 Bildern\n",
      "Trainable Params: 8,394,241\n",
      "âš¡ Mixed Precision (AMP) aktiviert.\n",
      "[01/15] Loss: 0.3267 | Val Loss: 0.3820 | AUROC: 0.9128 | AUPRC: 0.9187\n",
      "[02/15] Loss: 0.2831 | Val Loss: 0.3682 | AUROC: 0.9208 | AUPRC: 0.9237\n",
      "[03/15] Loss: 0.2597 | Val Loss: 0.3979 | AUROC: 0.9140 | AUPRC: 0.9186\n",
      "[04/15] Loss: 0.2479 | Val Loss: 0.3859 | AUROC: 0.9174 | AUPRC: 0.9217\n",
      "[05/15] Loss: 0.2374 | Val Loss: 0.4218 | AUROC: 0.9063 | AUPRC: 0.9141\n",
      "[06/15] Loss: 0.2111 | Val Loss: 0.4197 | AUROC: 0.9142 | AUPRC: 0.9199\n",
      "[07/15] Loss: 0.2010 | Val Loss: 0.4242 | AUROC: 0.9143 | AUPRC: 0.9208\n",
      "[08/15] Loss: 0.1940 | Val Loss: 0.4418 | AUROC: 0.9139 | AUPRC: 0.9205\n",
      "[09/15] Loss: 0.1885 | Val Loss: 0.4410 | AUROC: 0.9124 | AUPRC: 0.9195\n",
      "[10/15] Loss: 0.1886 | Val Loss: 0.4377 | AUROC: 0.9141 | AUPRC: 0.9210\n",
      "[11/15] Loss: 0.1865 | Val Loss: 0.4446 | AUROC: 0.9134 | AUPRC: 0.9203\n",
      "[12/15] Loss: 0.1850 | Val Loss: 0.4569 | AUROC: 0.9121 | AUPRC: 0.9194\n",
      "[13/15] Loss: 0.1858 | Val Loss: 0.4460 | AUROC: 0.9148 | AUPRC: 0.9214\n",
      "[14/15] Loss: 0.1852 | Val Loss: 0.4325 | AUROC: 0.9143 | AUPRC: 0.9211\n",
      "[15/15] Loss: 0.1842 | Val Loss: 0.4455 | AUROC: 0.9135 | AUPRC: 0.9203\n",
      "âœ… Training fertig! Bestes AUPRC: 0.9237\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import importlib\n",
    "\n",
    "# Eigene Module (Reload stellt sicher, dass Ã„nderungen Ã¼bernommen werden)\n",
    "import src.models.resnet_pcam_gpu as resnet_module\n",
    "importlib.reload(resnet_module)\n",
    "from src.models.resnet_pcam_gpu import ResNetConfig, ResNetPCam\n",
    "from src.datasets.dataloaders import get_pcam_dataloaders\n",
    "from src.training.utils_training import evaluate_binary_classifier\n",
    "\n",
    "# ==========================================\n",
    "# 1. KONFIGURATION (Windows-Safe!)\n",
    "# ==========================================\n",
    "# WICHTIG: Im Notebook auf Windows MUSS das 0 sein.\n",
    "# Wenn du Speed willst, mÃ¼sstest du das Skript als .py Datei im Terminal starten.\n",
    "NUM_WORKERS = 0 \n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸš€ Training auf: {DEVICE} | Workers: {NUM_WORKERS}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Die Trainings-Funktion (ROBUST FIX)\n",
    "# ==========================================\n",
    "def train_resnet_final(\n",
    "    tl_mode: str,\n",
    "    best_params: dict,\n",
    "    run_name: str,\n",
    "    num_epochs: int = 15,\n",
    "    save_dir: str = \"experiments/final_models\"\n",
    "):\n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n=== Starte Finales Training: {run_name} ({tl_mode}) ===\")\n",
    "    \n",
    "    # Params aus Optuna holen\n",
    "    batch_size = best_params[\"batch_size\"]\n",
    "    lr = best_params[\"lr\"]\n",
    "    weight_decay = best_params[\"weight_decay\"]\n",
    "    \n",
    "    # Loader neu erstellen (Full Dataset)\n",
    "    # Wichtig: pin_memory entfernt & num_workers=0 fÃ¼r Windows\n",
    "    loaders = get_pcam_dataloaders(\n",
    "        data_root=\"data/raw\",\n",
    "        batch_size=batch_size,\n",
    "        center_crop_size=64, \n",
    "        num_workers=NUM_WORKERS \n",
    "    )\n",
    "    \n",
    "    # Sicherheits-Check\n",
    "    n_train = len(loaders[\"train\"].dataset)\n",
    "    n_val = len(loaders[\"val\"].dataset)\n",
    "    print(f\"ðŸ“Š DATEN-CHECK: Training auf {n_train} Bildern | Validierung auf {n_val} Bildern\")\n",
    "\n",
    "    # Modell Setup\n",
    "    cfg = ResNetConfig(tl_mode=tl_mode, pretrained=True)\n",
    "    model = ResNetPCam(cfg).to(DEVICE)\n",
    "    \n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    print(f\"Trainable Params: {sum(p.numel() for p in trainable_params):,}\")\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(trainable_params, lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=2, verbose=True)\n",
    "    \n",
    "    # FIX: Klassischer Weg fÃ¼r Mixed Precision (funktioniert immer)\n",
    "    scaler = None\n",
    "    if DEVICE.type == 'cuda':\n",
    "        try:\n",
    "            scaler = torch.cuda.amp.GradScaler()\n",
    "            print(\"âš¡ Mixed Precision (AMP) aktiviert.\")\n",
    "        except Exception:\n",
    "            print(\"âš ï¸ Konnte AMP nicht aktivieren, laufe in Standard-PrÃ¤zision.\")\n",
    "    \n",
    "    best_val_auprc = 0.0 \n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for images, labels in loaders[\"train\"]:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.float().to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Training Step\n",
    "            if scaler:\n",
    "                # Klassischer Autocast Context\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    logits = model(images)\n",
    "                    loss = criterion(logits, labels)\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Standard ohne AMP\n",
    "                logits = model(images)\n",
    "                loss = criterion(logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Evaluation\n",
    "        train_loss /= len(loaders[\"train\"].dataset)\n",
    "        val_loss, val_auroc, val_auprc = evaluate_binary_classifier(\n",
    "            model, loaders[\"val\"], criterion, DEVICE\n",
    "        )\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(\n",
    "            f\"[{epoch:02d}/{num_epochs}] \"\n",
    "            f\"Loss: {train_loss:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f} | \"\n",
    "            f\"AUROC: {val_auroc:.4f} | \"\n",
    "            f\"AUPRC: {val_auprc:.4f}\"\n",
    "        )\n",
    "\n",
    "        if val_auprc > best_val_auprc:\n",
    "            best_val_auprc = val_auprc\n",
    "            torch.save(model.state_dict(), save_path / f\"{run_name}_best.pth\")\n",
    "            \n",
    "    print(f\"âœ… Training fertig! Bestes AUPRC: {best_val_auprc:.4f}\")\n",
    "    return model\n",
    "\n",
    "# ==========================================\n",
    "# 3. AUSFÃœHRUNG STARTEN\n",
    "# ==========================================\n",
    "\n",
    "# Definition der besten Parameter aus deinem Optuna-Lauf\n",
    "best_params_partial = {\n",
    "    \"lr\": 0.000462258900102083,\n",
    "    \"weight_decay\": 2.6587543983272695e-05,\n",
    "    \"batch_size\": 64\n",
    "}\n",
    "\n",
    "# B) Training PARTIAL\n",
    "print(\">>> Starte Training B: PARTIAL Fine-Tuning\")\n",
    "final_model_partial = train_resnet_final(\n",
    "    tl_mode=\"partial\",\n",
    "    best_params=best_params_partial,\n",
    "    run_name=\"resnet_partial_final\",\n",
    "    num_epochs=15\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
